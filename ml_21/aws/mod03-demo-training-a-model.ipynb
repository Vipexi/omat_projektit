{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration: Training a Model Using Amazon SageMaker - Companion Notebook\n",
    "\n",
    "This Jupyter notebook is the companion notebook for the Module 3 demonstration Training a Model Using Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this dataset\n",
    "\n",
    "This demonstration uses the [Wine Quality Data Set](https://archive.ics.uci.edu/ml/datasets/wine+quality) from the [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml).\n",
    "\n",
    "It contains information on wine quality.\n",
    "\n",
    "## Attribute information:\n",
    "\n",
    "For more information, read [Cortez et al., 2009].\n",
    "\n",
    "Input variables (based on physicochemical tests):\n",
    "1. fixed acidity\n",
    "2. volatile acidity\n",
    "3. citric acid\n",
    "4. residual sugar\n",
    "5. chlorides\n",
    "6. free sulfur dioxide\n",
    "7. total sulfur dioxide\n",
    "8. density\n",
    "9. pH\n",
    "10. sulphates\n",
    "11. alcohol\n",
    "    \n",
    "Output variable (based on sensory data):\n",
    "\n",
    "12. quality (score between 0 and 10)\n",
    "\n",
    "\n",
    "## Dataset attributions\n",
    "This dataset is from: \n",
    "Dua, D. and Graff, C. (2019). UCI Machine Learning Repository (http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science.\n",
    "\n",
    "P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.\n",
    "Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "df_wine = pd.read_csv(url,';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The quality values in the dataset contain the values 3-8. These values are mapped to 0-5 as target classes.\n",
    "\n",
    "XGBoost requires the training data to be in a single file. In the file, the target value must be the first column. \n",
    "\n",
    "Get the target column and move it to the first position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine['quality']=df_wine['quality'].map({3: 0, 4: 1, 5: 2, 6: 3, 7: 4, 8: 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_wine.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df_wine = df_wine[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('precision', 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model\n",
    "\n",
    "Start by showing a sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data\n",
    "\n",
    "You will start by splitting the dataset into two datasets. You will use one dataset for training, and you will split the other dataset again for use with validation and testing.\n",
    "\n",
    "You will use the *train_test_split function* from the *scikit-learn library*, which is a free machine learning library for Python. It has many algorithms and useful functions, such as the one you will use. \n",
    "\n",
    "- For more information about the function, see the [Train_test_split documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). \n",
    "- For more information about scikit-learn, see the [scikit-learn guide](https://scikit-learn.org/stable/).\n",
    "\n",
    "Because you don't have a lot of data, you want to make sure that the split datasets contain a representative amount of each class. Thus, you will use the *stratify* switch. Finally, you will use a random number so that you can repeat the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test_and_validate = train_test_split(df_wine, \n",
    "                                            test_size=0.2, \n",
    "                                            random_state=42, \n",
    "                                            stratify=df_wine['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, validate = train_test_split(test_and_validate, \n",
    "                                  test_size=0.5, \n",
    "                                  random_state=42, \n",
    "                                  stratify=test_and_validate['quality'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the size of each dataset based on the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(validate.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the distribution of the target from each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = train['quality'].value_counts()\n",
    "t2 = test['quality'].value_counts()\n",
    "t3 = validate['quality'].value_counts()\n",
    "result = pd.concat([t1,t2,t3], axis=1, sort=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading to Amazon S3\n",
    "\n",
    "XGboost will load the data for training from Amazon Simple Storage Service (Amazon S3). Thus, you must write the data to a comma-separated values (CSV) file, and then upload the file to Amazon S3.\n",
    "\n",
    "Start by setting up some variables for the S3 bucket, then create a function to upload the CSV file to Amazon S3. You can reuse this function.\n",
    "\n",
    "First, explore the function.\n",
    "\n",
    "Note the following line:\n",
    "\n",
    "`dataframe.to_csv(csv_buffer, header=False, index=False)`\n",
    "\n",
    "This line writes the pandas DataFrame (which was passed into the function) into the I/O buffer that's named *csv_buffer*. You use a buffer because you don't need to write the file locally.\n",
    "\n",
    "To stop the column headers from being written out, use `header=False`. To stop the pandas index from being output, use `index=False`.\n",
    "\n",
    "To write the csv_buffer to Amazon S3 as an object, use the PUT operation on the `object`, which is a property of the `bucket`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='c45317a617679l1523854t1w00381652629-sandboxbucket-3apxi73oxsw6'\n",
    "prefix='mod03-demo-training-a-model'\n",
    "train_file='wine_train.csv'\n",
    "test_file='wine_test.csv'\n",
    "validate_file='wine_validate.csv'\n",
    "whole_file='wine.csv'\n",
    "s3_resource = boto3.Session().resource('s3')\n",
    "\n",
    "def upload_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False )\n",
    "    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "upload_s3_csv(train_file, 'train', train)\n",
    "upload_s3_csv(test_file, 'test', test)\n",
    "upload_s3_csv(validate_file, 'validate', validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the estimator\n",
    "\n",
    "Now that the data in Amazon S3, you can train a model.\n",
    "\n",
    "The first step is to get the XGBoost container URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "import sagemaker\n",
    "role=sagemaker.get_execution_role()\n",
    "s3_output_location=\"s3://{}/{}/output/\".format(bucket,prefix)\n",
    "container = retrieve('xgboost',boto3.Session().region_name,'1.0-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only value to point out is the *num_class*, which is set to *6* to match the number of target classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams={\n",
    "    \"num_round\":\"40\",\n",
    "    \"num_class\":\"6\",\n",
    "    \"objective\":\"multi:softmax\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `estimator` function to set up the model. Here are a few parameters of interest:\n",
    "\n",
    "- **instance_count** - Defines how many instances will be used for training. You will use *one* instance.\n",
    "- **instance_type** - Defines the instance type for training. In this case, it's *ml.m4.xlarge*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model=sagemaker.estimator.Estimator(container,\n",
    "                                        role,\n",
    "                                        instance_count=1,\n",
    "                                        instance_type='ml.m5.xlarge',\n",
    "                                        output_path=s3_output_location,\n",
    "                                        hyperparameters=hyperparams,\n",
    "                                        sagemaker_session=sagemaker.Session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the input channels\n",
    "\n",
    "The estimator needs *channels* to feed data into the model. For training, the *train_channel* and the *validate_channel* will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/train/\".format(bucket,prefix,train_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "validate_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/validate/\".format(bucket,prefix,validate_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "data_channels = {'train': train_channel, 'validation': validate_channel}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Running `fit` will train the model.\n",
    "\n",
    "**Note:** This process can take up to 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(inputs=data_channels, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the metrics from the training job\n",
    "\n",
    "After the job is complete, you can view the metrics from the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=sagemaker.analytics.TrainingJobAnalytics(xgb_model._current_job_name, \n",
    "                                         metric_names = ['train:merror', \n",
    "                                                         'validation:merror']\n",
    "                                        )\n",
    "\n",
    "s_df=s.dataframe()\n",
    "s_df = s_df.iloc[:,1:3]\n",
    "s_df\n",
    "#(wrong cases)/#(all cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstration is now complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
