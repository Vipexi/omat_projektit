{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Companion Notebook\n",
    "\n",
    "This Jupyter notebook  is the companion notebook for the Module 4 demonstration, Creating a Forecast with Amazon Forecast.\n",
    "\n",
    "\n",
    "## Dataset attributions\n",
    "\n",
    "This notebook uses the following dataset: \n",
    "\n",
    "[Online Retail II Data Set](https://archive.ics.uci.edu/ml/datasets/Online+Retail+II)\n",
    "\n",
    "\n",
    "This dataset is from:\n",
    "Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructor notes\n",
    "\n",
    "In this demonstration, you will show how to create a forecast by using Amazon Forecast. Students will work through the same process as part of the lab. Thus, you could choose to use this demonstration as a summary of the lab, or you could also omit the demonstration, if needed.\n",
    "\n",
    "You could choose to deliver this demonstration in a few different ways:\n",
    "\n",
    "1. Run the entire notebook before the demonstration, and walk through the console instructions at the end of this notebook. See the section in this notebook titled **Reviewing the forecast creation in the console** (Recommended).\n",
    "\n",
    "2. Work through this notebook with the students. (**Note:** If you choose this option, it can take an hour to complete this demonstration.)\n",
    "\n",
    "3. Prepare the data by using this notebook, but create the forecast in the console. See the section in this notebook titled **Creating the forecast by using the console**. (**Note:** If you choose this option, it can take an hour to complete this demonstration.)\n",
    "\n",
    "Regardless of your choice, you should review this notebook in its entirety before you start the demonstration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook summary\n",
    "\n",
    "This notebook loads and preprocesses the Online Retail II dataset. The data is uploaded to Amazon Simple Storage Service (Amazon S3), where it is used to create a forecast by using Amazon Forecast. This notebook performs the following steps:\n",
    "\n",
    "- **Importing Python packages and creating functions** – Imports the packages that are used and creates helper functions\n",
    "- **Importing data** – Downloads and loads the data into a pandas DataFrame\n",
    "- **Pre-processing data** – Filters the data that's ready for training\n",
    "- **Generating training and test DataFrames** – Downsamples the data to a daily frequency, and splits the dataset into a training and test DataFrame\n",
    "- **Uploading to Amazon S3** – Uploads the DataFrames to Amazon S3 as comma-separated values (CSV) files\n",
    "- **Creating the Amazon Forecast dataset group** – Creates the project dataset group\n",
    "- **Creating the datasets** – Creates the datasets in the dataset group and waits for the import to complete\n",
    "- **Creating the predictor** – Trains the predictor by using the dataset group\n",
    "- **Getting accuracy metrics** – Displays the metrics for the predictor\n",
    "- **Creating the forecast** – Creates a test forecast\n",
    "- **Optional: Cleaning up** – Lists instructions for performing cleanup after the demonstration is complete\n",
    "\n",
    "This notebook takes 60–90 minutes to complete.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Python packages and creating functions\n",
    "\n",
    "The following code imports these packages:\n",
    "\n",
    "- *boto3* represents the AWS SDK for Python (Boto3), which is the Python library for AWS\n",
    "- *pandas* provides DataFrames for manipulating time series data\n",
    "- *matplotlib* provides plotting functions\n",
    "- *sagemaker* represents the API that's needed to work with Amazon SageMaker\n",
    "- *time*, *sys*, *os*, *io*, and *json* provide helper functions \n",
    "\n",
    "The code also creates two helper functions:\n",
    "\n",
    "- `upload_s3_csv` uploads pandas DataFrames to Amazon S3 as CSV files. The header is removed, but the index is *not* removed.\n",
    "- `StatusIndicator` provides a status function for long-running API calls to Forecast.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "bucket_name='c45317a617679l1523854t1w00381652629-sandboxbucket-3apxi73oxsw6'\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time, sys, os, io, json\n",
    "import sagemaker\n",
    "\n",
    "s3_resource = boto3.Session().resource('s3')\n",
    "\n",
    "def upload_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(filename, header=False, index=True)\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=True )\n",
    "    s3_resource.Bucket(bucket_name).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "class StatusIndicator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.previous_status = None\n",
    "        self.need_newline = False\n",
    "        \n",
    "    def update( self, status ):\n",
    "        if self.previous_status != status:\n",
    "            if self.need_newline:\n",
    "                sys.stdout.write(\"\\n\")\n",
    "            sys.stdout.write( status + \" \")\n",
    "            self.need_newline = True\n",
    "            self.previous_status = status\n",
    "        else:\n",
    "            sys.stdout.write(\".\")\n",
    "            self.need_newline = True\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def end(self):\n",
    "        if self.need_newline:\n",
    "            sys.stdout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data\n",
    "\n",
    "The following cell downloads the dataset, which is an Microsoft Excel file. This file is loaded into pandas as a DataFrame.\n",
    "\n",
    "This cell takes 1–2 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "session = boto3.Session()\n",
    "forecast = session.client(service_name='forecast') \n",
    "forecast_query = session.client(service_name='forecastquery')\n",
    "\n",
    "!aws s3 cp s3://aws-tc-largeobjects/CUR-TF-200-ACAIML-1/forecast/ . --recursive\n",
    "retail = pd.concat(pd.read_excel('online_retail_II.xlsx',sheet_name=None), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data\n",
    "\n",
    "The following cell completes these pre-processing steps:\n",
    "\n",
    "- Removes instances with missing values\n",
    "- Sets the index to the InvoiceDate feature\n",
    "- Only keeps instances that are from the United Kingdom\n",
    "- Only keeps instances that use the target stock code (21232)\n",
    "- Keeps instances where the price is greater than 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail = retail.dropna()\n",
    "retail['InvoiceDate'] = pd.to_datetime(retail.InvoiceDate)\n",
    "retail = retail.set_index('InvoiceDate')\n",
    "\n",
    "country_filter = ['United Kingdom']\n",
    "retail = retail[retail['Country'].isin(country_filter)]\n",
    "\n",
    "#stockcodes = ['ADJUST', 'ADJUST2', 'POST', 'M']\n",
    "#stockcodes = [21232,22423]\n",
    "stockcodes = [21232]\n",
    "retail = retail[retail.StockCode.isin(stockcodes)]\n",
    "\n",
    "retail = retail[retail['Price']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating training and test DataFrames\n",
    "\n",
    "The following cell:\n",
    "\n",
    "- Splits the data into a time series DataFrame and a related time series DataFrame.\n",
    "- Downsamples the data from multiple sales entries per day into a single daily value. The **Quantity** column is summed, and the mean is used for the **Price** column.\n",
    "- Splits the DataFrames into a training set that contains data from January 2010–October 2010, and a test set that contains data from November 2010–December 2010\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "retail_timeseries = retail[['StockCode','Quantity']]\n",
    "\n",
    "retail_timeseries = retail_timeseries.groupby('StockCode').resample('D').sum().reset_index().set_index(['InvoiceDate'])\n",
    "\n",
    "df_related_time_series = retail[['StockCode','Price']]\n",
    "df_related_time_series2 = df_related_time_series.groupby('StockCode').resample('D').mean().reset_index().set_index(['InvoiceDate'])\n",
    "df_related_time_series3 = df_related_time_series2.groupby('StockCode').pad()\n",
    "\n",
    "#df_related_time_series4 = df_related_time_series3.reset_index().set_index('InvoiceDate')\n",
    "\n",
    "# Select January to November for one dataframe.\n",
    "jan_to_oct = retail_timeseries['2009-12':'2011-10']\n",
    "nov_to_dec = retail_timeseries['2011-11':'2011-12']\n",
    "jan_to_oct_related = df_related_time_series2['2009-12':'2011-10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading to Amazon S3\n",
    "\n",
    "The following cell uploads the DataFrames to Amazon S3 by using the helper function that was created earlier.\n",
    "\n",
    "*Tip:* Update the prefix to something unique. If previous demos have not cleaned up completely, the notebook will fail. Changing the prefix will avoid this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prefix='mod_4_demo'\n",
    "train='retail_ts_train.csv'\n",
    "train_related='related_ts_train.csv'\n",
    "test='retail_ts_test.csv'\n",
    "\n",
    "key=prefix + '/forecast/' + train\n",
    "# key='lab_4_forecast_t/forecast/retail_time_series_train.csv'\n",
    "related_key = prefix + '/forecast/' + train_related\n",
    "# related_key='lab_4_forecast_t/forecast/related.csv'\n",
    "\n",
    "upload_s3_csv(train, 'forecast', jan_to_oct)\n",
    "upload_s3_csv(train_related, 'forecast', jan_to_oct_related)\n",
    "upload_s3_csv(test, 'forecast', nov_to_dec)\n",
    "\n",
    "dataset_frequency = \"D\" \n",
    "timestamp_format = \"yyyy-MM-dd\"\n",
    "\n",
    "# project = prefix\n",
    "dataset_name = prefix+'_ds'\n",
    "related_dataset_name = prefix+'_rds'\n",
    "dataset_group_name = prefix +'_dsg'\n",
    "\n",
    "s3_data_path = \"s3://\"+bucket_name+\"/\"+key\n",
    "s3_related_data_path = \"s3://\"+bucket_name+\"/\"+related_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%store prefix\n",
    "%store train\n",
    "%store test\n",
    "%store key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP HERE IF YOU ARE GOING TO DEMONSTRATE THE PROCESS OF CREATING FORECASTS IN THE CONSOLE\n",
    "\n",
    "For steps that outline how to create the forecast, see the **Creating the forecast by using the console** instructions at the end of this notebook.\n",
    "\n",
    "**Tip**: Remove this cell before you deliver the demonstration to students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Amazon Forecast dataset group\n",
    "\n",
    "The following cell creates the dataset group for the forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "create_dataset_group_response = forecast.create_dataset_group(DatasetGroupName=dataset_group_name,\n",
    "                                                              Domain=\"RETAIL\"\n",
    "                                                             )\n",
    "dataset_group_arn = create_dataset_group_response['DatasetGroupArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the datasets\n",
    "\n",
    "The following cell creates the time series and related datasets, and adds them to the dataset group.\n",
    "\n",
    "The cell will wait loop and display the status until the datasets are created.\n",
    "\n",
    "Note: Update the ARN below with the arn from your sandbox environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "iam = boto3.resource('iam')\n",
    "role_arn = iam.Role('ForecastRole').arn\n",
    "\n",
    "\n",
    "# This is the schema of the timeseries dataset.\n",
    "schema ={\n",
    "   \"Attributes\":[\n",
    "      {\n",
    "         \"AttributeName\":\"timestamp\",\n",
    "         \"AttributeType\":\"timestamp\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"item_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"demand\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      }\n",
    "   ]\n",
    "}\n",
    "\n",
    "time_series_response=forecast.create_dataset(\n",
    "                    Domain=\"RETAIL\",\n",
    "                    DatasetType='TARGET_TIME_SERIES',\n",
    "                    DatasetName=dataset_name,\n",
    "                    DataFrequency=dataset_frequency, \n",
    "                    Schema = schema\n",
    ")\n",
    "\n",
    "dataset_arn = time_series_response['DatasetArn']\n",
    "# forecast.describe_dataset(DatasetArn=dataset_arn)\n",
    "\n",
    "# Create the import job for the time series dataset\n",
    "dataset_import_job_name = 'EP_DSIMPORT_JOB_TARGET'\n",
    "data_source = {\"S3Config\" : {\"Path\":s3_data_path,\"RoleArn\": role_arn} }\n",
    "ds_import_job_response=forecast.create_dataset_import_job(DatasetImportJobName=dataset_import_job_name,\n",
    "                                                          DatasetArn=dataset_arn,\n",
    "                                                          DataSource= data_source,\n",
    "                                                          TimestampFormat=timestamp_format\n",
    "                                                         )\n",
    "\n",
    "ds_import_job_arn=ds_import_job_response['DatasetImportJobArn']\n",
    "\n",
    "# This is the schema of the related data, containing the price.\n",
    "related_schema ={\n",
    "   \"Attributes\":[\n",
    "      {\n",
    "         \"AttributeName\":\"timestamp\",\n",
    "         \"AttributeType\":\"timestamp\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"item_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"price\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      }\n",
    "   ]\n",
    "}\n",
    "\n",
    "related_time_series_response=forecast.create_dataset(\n",
    "                    Domain=\"RETAIL\",\n",
    "                    DatasetType='RELATED_TIME_SERIES',\n",
    "                    DatasetName=related_dataset_name,\n",
    "                    DataFrequency=dataset_frequency, \n",
    "                    Schema = related_schema\n",
    ")\n",
    "related_dataset_arn = related_time_series_response['DatasetArn']\n",
    "\n",
    "# forecast.describe_dataset(DatasetArn=related_dataset_arn)\n",
    "\n",
    "\n",
    "related_dataset_import_job_name = 'EP_DSIMPORT_JOB_TARGET_RELATED'\n",
    "\n",
    "related_data_source = {\"S3Config\" : {\"Path\":s3_related_data_path,\"RoleArn\": role_arn} }\n",
    "\n",
    "ds_related_import_job_response=forecast.create_dataset_import_job(DatasetImportJobName=related_dataset_import_job_name,\n",
    "                                                          DatasetArn=related_dataset_arn,\n",
    "                                                          DataSource= related_data_source,\n",
    "                                                          TimestampFormat=timestamp_format\n",
    "                                                         )\n",
    "\n",
    "ds_related_import_job_arn=ds_related_import_job_response['DatasetImportJobArn']\n",
    "\n",
    "# Add the time series and related dataset to the dataset group.\n",
    "forecast.update_dataset_group(DatasetGroupArn=dataset_group_arn, DatasetArns=[dataset_arn, related_dataset_arn])\n",
    "#forecast.update_dataset_group(DatasetGroupArn=dataset_group_arn, DatasetArns=[dataset_arn])\n",
    "\n",
    "# Wait for the related dataset to finish\n",
    "status_indicator = StatusIndicator()\n",
    "\n",
    "while True:\n",
    "    status = forecast.describe_dataset_import_job(DatasetImportJobArn=ds_related_import_job_arn)['Status']\n",
    "    status_indicator.update(status)\n",
    "    if status in ('ACTIVE', 'CREATE_FAILED'): break\n",
    "    time.sleep(10)\n",
    "\n",
    "status_indicator.end()\n",
    "\n",
    "# Wait for the time series dataset to finish - this typically takes longer than the related set.\n",
    "status_indicator = StatusIndicator()\n",
    "\n",
    "while True:\n",
    "    status = forecast.describe_dataset_import_job(DatasetImportJobArn=ds_import_job_arn)['Status']\n",
    "    status_indicator.update(status)\n",
    "    if status in ('ACTIVE', 'CREATE_FAILED'): break\n",
    "    time.sleep(10)\n",
    "\n",
    "status_indicator.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code stores the Amazon Resource Names (ARNs) for the forecast objects that were previously created. They can be loaded from other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store ds_import_job_arn\n",
    "%store dataset_arn\n",
    "%store dataset_group_arn\n",
    "%store related_dataset_arn\n",
    "%store ds_related_import_job_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the predictor\n",
    "\n",
    "The following cell creates the predictor by using the following parameters:\n",
    "\n",
    "- The forecast horizon is set to *30 days*.\n",
    "- *DeepAR+* is the selected algorithm. For more information, see [DeepARP+ Algorithm](https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-recipe-deeparplus.html) in the AWS Documentation.\n",
    "- Hyperparameters are specified for the algorithm. These hyperparameters were generated by running the forecast with **PerformHPO** set to *true*. This setting created a hyperparameter tuning job on the model, which produced the values that you see in the following cell.\n",
    "- A single backtest window for *30 days* is used.\n",
    "- The **input_data_config** field is set to the dataset group that was created previously.\n",
    "- Holidays in the United Kingdom are added as supplementary features.\n",
    "- A featurization pipeline is created for the price features. For more information, see the [Handling Missing Values](https://docs.aws.amazon.com/forecast/latest/dg/howitworks-missing-values.html) topic in the documentation.\n",
    "\n",
    "The cell will wait loop and display the status until the datasets are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictor_name= prefix+'_deeparp_algo'\n",
    "forecast_horizon = 90\n",
    "algorithm_arn = 'arn:aws:forecast:::algorithm/Deep_AR_Plus'\n",
    "\n",
    "training_parameters =  {'context_length': '172', \n",
    "                        'epochs': '500', \n",
    "                        'learning_rate': '0.00023391131837525837', \n",
    "                        'learning_rate_decay': '0.5', \n",
    "                        'likelihood': 'student-t', \n",
    "                        'max_learning_rate_decays': '0', \n",
    "                        'num_averaged_models': '1', \n",
    "                        'num_cells': '40', \n",
    "                        'num_layers': '2', \n",
    "                        'prediction_length': '30'}\n",
    "\n",
    "evaluation_parameters= {\"NumberOfBacktestWindows\": 1, \"BackTestWindowOffset\": 90}\n",
    "\n",
    "input_data_config = {\"DatasetGroupArn\": dataset_group_arn, \"SupplementaryFeatures\": [ {\"Name\": \"holiday\",\"Value\": \"UK\"} ]}\n",
    "                  \n",
    "featurization_config= {\"ForecastFrequency\": dataset_frequency,\n",
    "                      \"Featurizations\": \n",
    "                      [\n",
    "                          {\n",
    "                            \"AttributeName\": \"price\",\n",
    "                            \"FeaturizationPipeline\": [\n",
    "                                {\n",
    "                                    \"FeaturizationMethodName\": \"filling\",\n",
    "                                    \"FeaturizationMethodParameters\": {\n",
    "                                        \"middlefill\": \"median\",\n",
    "                                        \"backfill\": \"min\",\n",
    "                                        \"futurefill\": \"max\"               \n",
    "                                        }\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                      ]}\n",
    "\n",
    "\n",
    "create_predictor_response=forecast.create_predictor(PredictorName = predictor_name, \n",
    "                                                  AlgorithmArn = algorithm_arn,\n",
    "                                                  ForecastHorizon = forecast_horizon,\n",
    "                                                  PerformAutoML = False,\n",
    "                                                  PerformHPO = False,\n",
    "                                                  EvaluationParameters= evaluation_parameters, \n",
    "                                                  InputDataConfig = input_data_config,\n",
    "                                                  FeaturizationConfig = featurization_config #,\n",
    "#                                                   TrainingParameters = training_parameters\n",
    "                                                 )\n",
    "\n",
    "predictor_arn = create_predictor_response['PredictorArn']\n",
    "status_indicator = StatusIndicator()\n",
    "\n",
    "while True:\n",
    "    status = forecast.describe_predictor(PredictorArn=predictor_arn)['Status']\n",
    "    status_indicator.update(status)\n",
    "    if status in ('ACTIVE', 'CREATE_FAILED'): break\n",
    "    time.sleep(10)\n",
    "\n",
    "status_indicator.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = forecast.describe_predictor(PredictorArn=predictor_arn)\n",
    "print(f['TrainingParameters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting accuracy metrics\n",
    "\n",
    "The next cell prints the accuracy metrics for the predictor that was just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.get_accuracy_metrics(PredictorArn=predictor_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the forecast\n",
    "\n",
    "The following cell creates a forecast from the predictor that was created previously. \n",
    "\n",
    "The predictor and forecast ARN values are stored so that they can be retreived from the lab notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "forecast_Name= prefix+'_deeparp_algo_forecast'\n",
    "create_forecast_response=forecast.create_forecast(ForecastName=forecast_Name,\n",
    "                                                  PredictorArn=predictor_arn)\n",
    "forecast_arn = create_forecast_response['ForecastArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store forecast_arn\n",
    "%store predictor_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "status_indicator = StatusIndicator()\n",
    "while True:\n",
    "    status = forecast.describe_forecast(ForecastArn=forecast_arn)['Status']\n",
    "    status_indicator.update(status)\n",
    "    if status in ('ACTIVE', 'CREATE_FAILED'): break\n",
    "    time.sleep(10)\n",
    "\n",
    "status_indicator.end()\n",
    "\n",
    "print(forecast_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates a quick forecast as a test, which is useful for troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "forecast_response = forecast_query.query_forecast(\n",
    "    ForecastArn=forecast_arn,\n",
    "    Filters={\"item_id\":\"21232\"}\n",
    ")\n",
    "print(forecast_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Cleaning up\n",
    "\n",
    "To delete the forecast that was generated by using this notebook, select the following cell, change the cell to code by pressing Y, and then run them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forecast.delete_forecast(ForecastArn=forecast_arn)\n",
    "time.sleep(60)\n",
    "\n",
    "forecast.delete_predictor(PredictorArn=predictor_arn)\n",
    "time.sleep(60)\n",
    "\n",
    "forecast.delete_dataset_import_job(DatasetImportJobArn=ds_import_job_arn)\n",
    "time.sleep(60)\n",
    "\n",
    "forecast.delete_dataset(DatasetArn=dataset_arn)\n",
    "time.sleep(60)\n",
    "\n",
    "forecast.delete_dataset_group(DatasetGroupArn=dataset_group_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration steps complete!\n",
    "\n",
    "If you walked through the code with students, you can stop here and delete the following cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING THE FORECAST BY USING THE CONSOLE: \n",
    "# STEP-BY-STEP INSTRUCTIONS\n",
    "\n",
    "The following instructions demonstrate how to complete the demonstration by using the console. \n",
    "\n",
    "**Note:** You should have ran the previous cells to create the dataset *and* upload the data to Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Creating the dataset group\n",
    "\n",
    "1. On the AWS Management Console, on the **Services** menu, choose **Amazon Forecast**.\n",
    "2. Choose **Create dataset group**, and in the form, provide these values.\n",
    "\n",
    "    - **Dataset group name**: Enter an appropriate name\n",
    "    - **Forecasting domain**: *Retail*\n",
    "\n",
    "Your screen should look similiar to the following example:\n",
    "\n",
    "![Screen capture of the Creating dataset group task](images/mod4-demo.PNG)\n",
    "\n",
    "3. Choose **Next**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Creating the target time series dataset\n",
    "\n",
    "1. In the **Dataset name** box, enter an appropriate name.\n",
    "2. Update the **Data schema** by moving the timestamp to the first position, like in the following example:\n",
    "\n",
    "![Screen capture of the Creating target time series dataset task](images/mod4-demo2.PNG)\n",
    "\n",
    "3. Choose **Next**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a note of the S3 bucket where the data is located by running the following cell. You will need this information in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f's3://{bucket_name}/{prefix}/forecast/{train}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Importing target time series data\n",
    "\n",
    "1. In the **Dataset import details** form, provide these values.\n",
    "\n",
    "    - **Dataset import name**: Enter an appropriate name\n",
    "    - **Timestamp format**: `yyyy-MM-dd`\n",
    "    - **IAM Role**: Select the existing role (**Note:** This role was created as part of creating the sandbox environment)\n",
    "    - **Dataset location**: Enter the path to the S3 bucket\n",
    "\n",
    "The screen should look like the following example:\n",
    "\n",
    "![Screen capture of the Importing the target time series data task](images/mod4-demo3.PNG)\n",
    "\n",
    "2. Choose **Start import**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** It takes 5–20 minutes for the data to be imported. *Make sure that the import is completed before you proceed.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Training a predictor\n",
    "\n",
    "1. Under the **Train a predictor** section, choose **Start**.\n",
    "2. Provide the following values.\n",
    "\n",
    "    - **Predictor name**: Enter an appropriate name\n",
    "    - **Forecast horizon**: `90` \n",
    "    - **Algorithm selection**: *Manual*\n",
    "    - **Algorithm**: *Deep_AR_Plus*\n",
    "    - **Country for holidays**: *United Kingdom* \n",
    "\n",
    "Your screen should look similiar to the following:\n",
    "\n",
    "![Screen capture of the Training a predictor task](images/mod4-demo4.PNG)\n",
    "\n",
    "3. Choose **Train predictor**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** It takes between 20–40 minutes to train the predictor. *Make sure that the predictor training has finished before you continue.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Generating the forecast\n",
    "\n",
    "1. In the **Generate forecasts** section, choose **Start**, and provide these values.\n",
    "    - **Forecast name**: Enter an appropriate name\n",
    "    - **Predictor**: Select the predictor that you just created \n",
    "    - **Forecast types**: Leave this box empty\n",
    "\n",
    "You screen should look like the following example:\n",
    "\n",
    "![Screen capture of the Generating the forecast task](images/mod4-demo5.PNG)\n",
    "\n",
    "2. Choose **Create a forecast**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** It takes between 20–40 minutes to create the forecast. *Make sure that the forecast has been created before you continue.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Looking up the forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose **Lookup forecast** and provide these values.\n",
    "    - **Forecast**: Select the forecast that you just created \n",
    "    - **Start date**: `2011/10/02`\n",
    "    - **End date**: `2011/12/31`\n",
    "    - **Value**: `21232`\n",
    "\n",
    "Your screen should look similiar to the following example:\n",
    "\n",
    "![Screen capture of the Looking up the forecast task](images/mod4-demo6.PNG)\n",
    "\n",
    "6. Choose **Get Forecast**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a few seconds, you should get a forecast that's similar to the following example:\n",
    "\n",
    "![Screen capture of the forecast results](images/mod4-demo7.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewing the creation of the forecast in the console \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Reviewing the datasets\n",
    "\n",
    "1. Choose **View dataset groups**.\n",
    "2. From the list of dataset groups, select **mod_4_demo_dsg**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have previously created the forecast, your Amazon Forecast dashboard should look like the following example:\n",
    "    \n",
    "![Amazon Forecast dashboard](images/mod4-demo8.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Choose **View datasets**.\n",
    "\n",
    "You screen should look like the following example:\n",
    "\n",
    "![Dataset View](images/mod4-demo9.PNG)\n",
    "\n",
    "4. Choose **mod_4_demo_ds**, which is the time series dataset.\n",
    "5. With the students, review the **Dataset import field statistics** and **Schema** sections.\n",
    "6. To return to the dataset dashboard, choose **Dashboard**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Reviewing the predictor\n",
    "\n",
    "1. Choose **View predictors**.\n",
    "2. From the list of predictors, select **mod_4_demo_deeparp_algo**.\n",
    "\n",
    "Your screen should look similiar to the following example:\n",
    "\n",
    "![Predictor overview](images/mod4-demo9.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Review the **Forecast Configurations** section, and point out the **Forecast horizon**, **Forecast frequency**, and **Country for holidays** settings.\n",
    "4. Review the **Predictor metrics** section.\n",
    "5. To return to the dataset dashboard, choose **Dashboard**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Looking up a forecast\n",
    "\n",
    "1. Choose **Lookup forecast** and provide these values.\n",
    "    - **Forecast**: *mod_4_demo_deeparp_algo_forecast*\n",
    "    - **Start date**: `2011/10/02`\n",
    "    - **End date**: `2011/12/31`\n",
    "    - **Value**: `21232`\n",
    "6. Choose **Get Forecast**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a few seconds, you should get a forecast that's similar to the following example:\n",
    "\n",
    "![Screen capture of the forecast results](images/mod4-demo7.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Console demonstration steps complete!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
