{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration: Optimizing Amazon SageMaker Hyperparameters - Companion Notebook\n",
    "\n",
    "This Jupyter notebook is the companion notebook for the Module 3 demonstration Optimizing Amazon SageMaker Hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this dataset\n",
    "\n",
    "This demonstration uses the [Wine Data Set](https://archive.ics.uci.edu/ml/datasets/Wine), which was obtained from the [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml). \n",
    "\n",
    "It contains information about wine quality.\n",
    "\n",
    "## Attribute information\n",
    "\n",
    "For more information, read [Cortez et al., 2009].\n",
    "\n",
    "Input variables (based on physicochemical tests):\n",
    "1. fixed acidity\n",
    "2. volatile acidity\n",
    "3. citric acid\n",
    "4. residual sugar\n",
    "5. chlorides\n",
    "6. free sulfur dioxide\n",
    "7. total sulfur dioxide\n",
    "8. density\n",
    "9. pH\n",
    "10. sulphates\n",
    "11. alcohol\n",
    "    \n",
    "Output variable (based on sensory data):\n",
    "\n",
    "12. quality (score between 0 and 10)\n",
    "\n",
    "For more information about this dataset, see the [Wine Quality Data Set webpage](https://archive.ics.uci.edu/ml/datasets/wine+quality).\n",
    "\n",
    "## Dataset attributions\n",
    "This dataset was obtained from: Dua, D. and Graff, C. (2019). UCI Machine Learning Repository (http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science.\n",
    "\n",
    "P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.\n",
    "Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "df_wine = pd.read_csv(url,';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The quality values in the dataset contain values from 3-8. These values are mapped to 0-5 as target classes.\n",
    "\n",
    "XGBoost requires the training data to be in a single file. In the file, the target value must be the first column. \n",
    "\n",
    "Get the target column and move it to the first position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine['quality']=df_wine['quality'].map({3: 0, 4: 1, 5: 2, 6: 3, 7: 4, 8: 5})\n",
    "\n",
    "cols = df_wine.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df_wine = df_wine[cols]\n",
    "df_wine.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data\n",
    "\n",
    "You will start by splitting the dataset into two datasets. You will use one dataset for training, and you will split the other dataset again for use with validation and testing.\n",
    "\n",
    "You will use the *train_test_split function* from the *scikit-learn library*, which is a free machine learning library for Python. It has many algorithms and useful functions, such as the one you will use in this demonstration. \n",
    "\n",
    "- For more information about the function, see the [Train_test_split documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). \n",
    "- For more information about scikit-learn, see the [scikit-learn guide](https://scikit-learn.org/stable/).\n",
    "\n",
    "Because you don't have a lot of data, you want to make sure that the split datasets contain a representative amount of each class. Thus, you will use the *stratify* switch. Finally, you will use a random number so that you can repeat the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test_and_validate = train_test_split(df_wine, \n",
    "                                            test_size=0.2, \n",
    "                                            random_state=42, \n",
    "                                            stratify=df_wine['quality'])\n",
    "\n",
    "test, validate = train_test_split(test_and_validate, \n",
    "                                  test_size=0.5, \n",
    "                                  random_state=42, \n",
    "                                  stratify=test_and_validate['quality'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading to Amazon S3\n",
    "\n",
    "XGboost will load the data for training from Amazon Simple Storage Service (Amazon S3). Thus, you must write the data to a comma-separated values (CSV) file, and then upload the file to Amazon S3.\n",
    "\n",
    "Start by setting up some variables for the S3 bucket, then create a function to upload the CSV file to Amazon S3. You can reuse this function.\n",
    "\n",
    "First, explore the function.\n",
    "\n",
    "Note the following line:\n",
    "\n",
    "`dataframe.to_csv(csv_buffer, header=False, index=False)`\n",
    "\n",
    "This line writes the pandas DataFrame (which was passed into the function) into the I/O buffer that's named *csv_buffer*. You use a buffer because you don't need to write the file locally.\n",
    "\n",
    "To stop the column headers from being written out, use `header=False`. To stop the pandas index from being output, use `index=False`.\n",
    "\n",
    "To write the csv_buffer to Amazon S3 as an object, use the PUT operation on the `object`, which is a property of the `bucket`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "import os\n",
    "\n",
    "bucket='c45317a617679l1523854t1w00381652629-sandboxbucket-3apxi73oxsw6'\n",
    "prefix='mod03-demo-hyperparam'\n",
    "train_file='wine_train.csv'\n",
    "test_file='wine_test.csv'\n",
    "validate_file='wine_validate.csv'\n",
    "whole_file='wine.csv'\n",
    "s3_resource = boto3.Session().resource('s3')\n",
    "\n",
    "def upload_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False )\n",
    "    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "upload_s3_csv(train_file, 'train', train)\n",
    "upload_s3_csv(test_file, 'test', test)\n",
    "upload_s3_csv(validate_file, 'validate', validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the estimator\n",
    "\n",
    "Now that the data in Amazon S3, you can train a model.\n",
    "\n",
    "The first step is to get the XGBoost container URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.image_uris import retrieve\n",
    "container = retrieve('xgboost',boto3.Session().region_name,'1.0-1')\n",
    "\n",
    "s3_output_location=\"s3://{}/{}/output/\".format(bucket,prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only value to point out is the *num_class*, which is set to *6* to match the number of target classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams={\n",
    "    \"num_round\":\"40\",\n",
    "    \"num_class\":\"6\",\n",
    "    \"objective\":\"multi:softmax\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `estimator` function to set up the model. Here are a few parameters of interest:\n",
    "\n",
    "- **train_instance_count** - Defines how many instances will be used for training. You will use *one* instance.\n",
    "- **train_instance_type** - Defines the instance type for training. In this case, it's *ml.m4.xlarge*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model=sagemaker.estimator.Estimator(container,\n",
    "                                        sagemaker.get_execution_role(),\n",
    "                                        instance_count=1,\n",
    "                                        instance_type='ml.m5.xlarge',\n",
    "                                        output_path=s3_output_location,\n",
    "                                        hyperparameters=hyperparams,\n",
    "                                        sagemaker_session=sagemaker.Session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the input channels\n",
    "\n",
    "The estimator needs *channels* to feed data into the model. For training, you will use the *train_channel* and the *validate_channel*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/train/\".format(bucket,prefix,train_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "validate_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/validate/\".format(bucket,prefix,validate_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "data_channels = {'train': train_channel, 'validation': validate_channel}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "               \n",
    "Next, you must specify the hyperparameters that you want to tune, in addition to the ranges that you must select for each parameter.\n",
    "\n",
    "The hyperparameters that have the largest effect on XGBoost objective metrics are: \n",
    "\n",
    "- alpha\n",
    "- min_child_weight\n",
    "- subsample\n",
    "- eta\n",
    "- num_round \n",
    "\n",
    "For more information about the recommended tuning ranges, see [Tune an XGBoost Model](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html) in the AWS Documentation.\n",
    "\n",
    "For this demonstration, you will use a *subset* of values. These values were obtained by running the tuning job with the full range, then minimizing the range so that you can use fewer iterations to get better performance. Though this practice isn't strictly realistic, it prevents you from waiting several hours for the tuning job to complete.\n",
    "\n",
    "```\n",
    "hyperparameter_ranges = {'alpha': ContinuousParameter(0, 1000),\n",
    "                         'eta': ContinuousParameter(0.1, 0.5),\n",
    "                         'min_child_weight': ContinuousParameter(1, 120),\n",
    "                         'subsample': ContinuousParameter(0.5, 1),\n",
    "                         'num_round': IntegerParameter(1,4000)}\n",
    "```\n",
    "\n",
    "\n",
    "You must specify how you are rating the model. You could use several different objective metrics, a subset of which applies to a binary classifcation problem. Because the evaluation metric is **merror**, you set the objective to *merror*.\n",
    "\n",
    "```\n",
    "objective_metric_name = 'validation:merror'\n",
    "objective_type = 'Minimize'\n",
    "```\n",
    "\n",
    "Finally, run the tuning job.\n",
    "\n",
    "```\n",
    "tuner = HyperparameterTuner(xgb_model,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=40,\n",
    "                            max_parallel_jobs=1,\n",
    "                            objective_type=objective_type))\n",
    "\n",
    "tuner.fit(inputs=data_channels, include_cls_metadata=False)\n",
    "tuner.wait()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.parameter import (\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    IntegerParameter,\n",
    "    ParameterRange,\n",
    ")\n",
    "from sagemaker.amazon.hyperparameter import Hyperparameter\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "container = retrieve('xgboost',boto3.Session().region_name,'1.0-1')\n",
    "\n",
    "hyperparameter_ranges = {'alpha': ContinuousParameter(0, 1000),\n",
    "                         'eta': ContinuousParameter(0.1, 0.5),\n",
    "                         'min_child_weight': ContinuousParameter(1, 120),\n",
    "                         'subsample': ContinuousParameter(0.5, 1),\n",
    "                         'num_round': IntegerParameter(1,4000)}\n",
    "\n",
    "objective_metric_name = 'validation:merror'\n",
    "objective_type = 'Minimize'\n",
    "\n",
    "tuner = HyperparameterTuner(xgb_model,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=40,\n",
    "                            max_parallel_jobs=1,\n",
    "                            objective_type=objective_type)\n",
    "\n",
    "tuner.fit(inputs=data_channels, include_cls_metadata=False)\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training job is finished, check the job and make sure that it completed successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.client('sagemaker').describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the job is complete, there should be 10 completed jobs. One of the jobs should be marked as the best.\n",
    "\n",
    "You can examine the metrics by getting *HyperparameterTuningJobAnalytics* and loading that data into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from sagemaker.analytics import HyperparameterTuningJobAnalytics\n",
    "\n",
    "tuner_analytics = HyperparameterTuningJobAnalytics(tuner.latest_tuning_job.name, sagemaker_session=sagemaker.Session())\n",
    "\n",
    "df_tuning_job_analytics = tuner_analytics.dataframe()\n",
    "\n",
    "# Sort the tuning job analytics by the final metrics value\n",
    "df_tuning_job_analytics.sort_values(\n",
    "    by=['FinalObjectiveValue'],\n",
    "    inplace=True,\n",
    "    ascending=False if tuner.objective_type == \"Maximize\" else True)\n",
    "\n",
    "# Show detailed analytics for the top 5 models\n",
    "df_tuning_job_analytics.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to see the hyperparameters that were used for each job, along with the score. You could use those parameters and create a model, or you can get the best model from the hyperparameter tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attached_tuner = HyperparameterTuner.attach(tuner.latest_tuning_job.name, sagemaker_session=sagemaker.Session())\n",
    "best_training_job = attached_tuner.best_training_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you must attach the estimator to the best training job and create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "algo_estimator = Estimator.attach(best_training_job)\n",
    "\n",
    "best_algo_model = algo_estimator.create_model(env={'SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT':\"text/csv\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstration is now complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
